{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading training files, removing NaN's and dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Brillantaisia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>genus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Acanthaceae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DATE</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LETTERS</td>\n",
       "      <td>IUCN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>List</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Threatened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Circa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Survive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>draw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>influences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>hardcore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>experimental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>emo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>progressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917872</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Affeln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917874</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Altenaffeln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917876</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Blintrop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917883</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>advowson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917901</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>conventuals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917938</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Kamptz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917942</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Theilnahme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917944</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>adlichen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917945</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Klosterstellen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917973</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Herzogtumer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918042</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Constructivists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918045</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Rodchenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918064</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Mathilde's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918073</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Geiselhoring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918080</th>\n",
       "      <td>DATE</td>\n",
       "      <td>12 December 1859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918095</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Neufchatel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918160</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Belik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918167</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Schmucker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918172</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Neuhorst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918217</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Neuill√©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918259</th>\n",
       "      <td>DATE</td>\n",
       "      <td>October 1865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918267</th>\n",
       "      <td>DATE</td>\n",
       "      <td>14 January 1867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918278</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Neukirch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918313</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Eliswil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918316</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Christerode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918360</th>\n",
       "      <td>LETTERS</td>\n",
       "      <td>vun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918361</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Neileininge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918364</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Neulliac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918368</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>Neulieg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9918426</th>\n",
       "      <td>PLAIN</td>\n",
       "      <td>perceptible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>484145 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           class            before\n",
       "0          PLAIN     Brillantaisia\n",
       "1          PLAIN                is\n",
       "2          PLAIN                 a\n",
       "3          PLAIN             genus\n",
       "4          PLAIN                of\n",
       "5          PLAIN             plant\n",
       "6          PLAIN                in\n",
       "7          PLAIN            family\n",
       "8          PLAIN       Acanthaceae\n",
       "9          PUNCT                 .\n",
       "10          DATE              2006\n",
       "11       LETTERS              IUCN\n",
       "12         PLAIN               Red\n",
       "13         PLAIN              List\n",
       "15         PLAIN        Threatened\n",
       "16         PLAIN           Species\n",
       "18         PLAIN             Circa\n",
       "19         PLAIN           Survive\n",
       "20         PLAIN              draw\n",
       "21         PLAIN        influences\n",
       "22         PLAIN              from\n",
       "23         PLAIN              soft\n",
       "24         PLAIN              rock\n",
       "25         PUNCT                 ,\n",
       "26         PLAIN              post\n",
       "27         PLAIN          hardcore\n",
       "29         PLAIN      experimental\n",
       "32         PLAIN               emo\n",
       "34         PLAIN       progressive\n",
       "37         PLAIN               art\n",
       "...          ...               ...\n",
       "9917872    PLAIN            Affeln\n",
       "9917874    PLAIN       Altenaffeln\n",
       "9917876    PLAIN          Blintrop\n",
       "9917883    PLAIN          advowson\n",
       "9917901    PLAIN       conventuals\n",
       "9917938    PLAIN            Kamptz\n",
       "9917942    PLAIN        Theilnahme\n",
       "9917944    PLAIN          adlichen\n",
       "9917945    PLAIN    Klosterstellen\n",
       "9917973    PLAIN       Herzogtumer\n",
       "9918042    PLAIN   Constructivists\n",
       "9918045    PLAIN         Rodchenko\n",
       "9918064    PLAIN        Mathilde's\n",
       "9918073    PLAIN      Geiselhoring\n",
       "9918080     DATE  12 December 1859\n",
       "9918095    PLAIN        Neufchatel\n",
       "9918160    PLAIN             Belik\n",
       "9918167    PLAIN         Schmucker\n",
       "9918172    PLAIN          Neuhorst\n",
       "9918217    PLAIN           Neuill√©\n",
       "9918259     DATE      October 1865\n",
       "9918267     DATE   14 January 1867\n",
       "9918278    PLAIN          Neukirch\n",
       "9918313    PLAIN           Eliswil\n",
       "9918316    PLAIN       Christerode\n",
       "9918360  LETTERS               vun\n",
       "9918361    PLAIN       Neileininge\n",
       "9918364    PLAIN          Neulliac\n",
       "9918368    PLAIN           Neulieg\n",
       "9918426    PLAIN       perceptible\n",
       "\n",
       "[484145 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH=\"C:\\\\Users\\\\Dani-Desktop\\\\Desktop\\\\NLP DATA\\\\en_train.csv\"\n",
    "df= pd.read_csv(PATH)\n",
    "exclude_classes_car = ['ELECTRONIC']\n",
    "df_c = df.loc[df['class'].isin(exclude_classes_car) == False]\n",
    "df_x= df_c.iloc[:-1,2:4]\n",
    "df_x = df_x.drop_duplicates()\n",
    "df_x=df_x.dropna()\n",
    "df_x=df_x.drop(2680666) # dropping the max sized word here, ALWAYS DO THIS\n",
    "df_x=df_x.drop(2595490)\n",
    "df_x=df_x.drop(2972460)\n",
    "display(df_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PLAIN' 'PUNCT' 'DATE' 'LETTERS' 'CARDINAL' 'VERBATIM' 'DECIMAL' 'MEASURE'\n",
      " 'MONEY' 'ORDINAL' 'TIME' 'DIGIT' 'FRACTION' 'TELEPHONE' 'ADDRESS']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Brillantaisia'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_classes=df_x[\"class\"].unique()\n",
    "lenx=len(unique_classes)\n",
    "\n",
    "print (unique_classes)\n",
    "\n",
    "##GETTING ALL THE WORD IN THE VOCAB\n",
    "words=df_x.iloc[:,1]\n",
    "\n",
    "display(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot-Vec for all our Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HotVec for our CLasses\n",
    "class_hot_vec= np.zeros((lenx,lenx),int) \n",
    "np.fill_diagonal(class_hot_vec,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Number of Words after removing NaN's and dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484145\n"
     ]
    }
   ],
   "source": [
    "print (len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(s, el):\n",
    "    for i in s.index:\n",
    "        if s[i] == el: \n",
    "            return i\n",
    "    return None\n",
    "#words=words.drop(2680666)  #words=words.drop(2680666)\n",
    "#print (find(words, \"groupsAdenosinergicAdrenergicCannabinoidergicCholinergicDopaminergicGABAergicGlycinergicHistaminergicMelatonergicMonoaminergicOpioidergic\"))\n",
    "#print (find(words, \"TomotsuneAritsuneTsuneyasuYasutomoYasukageTomoyasuMunetomoYasumuneNoriyasuTomoyasuYasukageYasuyoshi\"))\n",
    "#print (find(words, \"Pass√©SouvenirRomanceLiedAgitatoAdieuReverieCapriceInqui√©tudeIntermezzoOp\"))\n",
    "#print (find(words, \"BalianBalogoBalungisanBinangonanBulacanBulawanCalapeDalamaFatima\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max word :  LevushkovskyPlastunovskyDyadkovskyBryukhovetskyVedmedovskyPlatmyrovskyPashkovskyKushchevskyKyslyakovskyIvanovskyKonelovskySerhiyevskyDonskyKrylovskyKanivskyBaturynskyPopovychevskyVasyurynskyNezamaikovskyIrkliyevskyShcherbynovskyTytarovskyShkurynskyKurenevskyRohovskyKorsunksyKalnybolotskyHumanskyDerevyantsovskyStebliyivsky\n",
      "max word len:  323\n"
     ]
    }
   ],
   "source": [
    "#words=words.drop(2680666) # dropping the max sized word here, ALWAYS DO THIS\n",
    "#words=words.drop(2595490)\n",
    "#words=words.drop(2972460)\n",
    "#words=words.drop(888311)\n",
    "max_word=[str(i) for i in words]\n",
    "max_word=sorted(max_word,key=len)\n",
    "x=-1\n",
    "max_word_len=len(max_word[x])\n",
    "#len(sorted_word_list[9913279-3])max_word_len\n",
    "print (\"max word : \", max_word[x])\n",
    "print (\"max word len: \", max_word_len)\n",
    "\n",
    "\n",
    "#Following is the Max word:\n",
    "#groupsAdenosinergicAdrenergicCannabinoidergicCholinergicDopaminergicGABAergicGlycinergicHistaminergicMelatonergicMonoaminergicOpioidergic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all the unique characters in our list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484145\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['’á',\n",
       " 'ËøΩ',\n",
       " 'Â∞æ',\n",
       " 'ŸÄ',\n",
       " '„ÅÇ',\n",
       " '‰øÆ',\n",
       " 'ÈôÑ',\n",
       " 'Êª∏',\n",
       " '‡∏Ñ',\n",
       " 'ÈÅ†',\n",
       " 'Îûå',\n",
       " '‡¶ú',\n",
       " '4',\n",
       " 'ÈäÄ',\n",
       " 'y',\n",
       " '·ªØ',\n",
       " 'Á¥Ñ',\n",
       " '—Ü',\n",
       " 'Ë£Ö',\n",
       " 'ÎÇò',\n",
       " 'È†ì',\n",
       " 'ÊúÉ',\n",
       " 'Âêë',\n",
       " 'Á∫ø',\n",
       " '„Åú',\n",
       " 'Á≠â',\n",
       " 'Èóò',\n",
       " 'Ëëâ',\n",
       " '–ó',\n",
       " 'ÂæÑ',\n",
       " 'Â•¥',\n",
       " 'ÈôÄ',\n",
       " 'Èú≤',\n",
       " '·õÅ',\n",
       " '„Ç´',\n",
       " 'ËßÜ',\n",
       " 'Â≤≥',\n",
       " '„Åæ',\n",
       " 'Â≥∂',\n",
       " '–≤',\n",
       " 'Ë≤©',\n",
       " 'Ïï®',\n",
       " 'Êµô',\n",
       " '‰∫∫',\n",
       " 'Èãò',\n",
       " 'ÈúÄ',\n",
       " 'Á∫¶',\n",
       " 'Â∞ã',\n",
       " 'Áü•',\n",
       " '‰πç',\n",
       " 'Á¥´',\n",
       " '„Å™',\n",
       " 'È£õ',\n",
       " '„Åö',\n",
       " 'Â∑£',\n",
       " 'Í≥Ñ',\n",
       " 'Ê¶ä',\n",
       " 'Âßã',\n",
       " 'Â•î',\n",
       " 'Œï',\n",
       " 'Âä™',\n",
       " 'Èüì',\n",
       " '–î',\n",
       " 'Ê°Ç',\n",
       " 'ÈÅá',\n",
       " 'Íµ¨',\n",
       " 'Ê≠ª',\n",
       " 'Âãá',\n",
       " '–ò',\n",
       " 'Êü≥',\n",
       " 'Îèå',\n",
       " 'Â±ë',\n",
       " '·õè',\n",
       " 'Ïô∏',\n",
       " '‰Ωê',\n",
       " 'ÂΩ±',\n",
       " 'Èõ™',\n",
       " '–¶',\n",
       " 'Êí≤',\n",
       " '·àï',\n",
       " 'ÂÖ∏',\n",
       " '–µ',\n",
       " '·π¨',\n",
       " 'Âõæ',\n",
       " 'ÁΩ∞',\n",
       " 'Ëëµ',\n",
       " 'Ï∞Ω',\n",
       " 'Ë¶ß',\n",
       " 'Ë°¢',\n",
       " 'Âõ£',\n",
       " 'Ëò≠',\n",
       " '·àõ',\n",
       " 'Á¨†',\n",
       " 'Â¥î',\n",
       " '⁄æ',\n",
       " 'Ï¶ê',\n",
       " 'ƒí',\n",
       " 'ÌÅê',\n",
       " '·â∞',\n",
       " 'ÈÅÆ',\n",
       " 'Êº¢',\n",
       " 'ÈÉ°',\n",
       " 'ÁöÑ',\n",
       " 'ÂéÇ',\n",
       " '·ªü',\n",
       " 'Êâã',\n",
       " '‡Æ≥',\n",
       " 'Ê°É',\n",
       " 'ÂùÇ',\n",
       " 'Êòü',\n",
       " '„Å¢',\n",
       " 'œµ',\n",
       " 'Êßº',\n",
       " 'Áªö',\n",
       " '–∂',\n",
       " 'Îì±',\n",
       " 'ÈòÆ',\n",
       " 'Áí©',\n",
       " '√ê',\n",
       " 'Êöé',\n",
       " 'Ïòà',\n",
       " 'Â™í',\n",
       " 'R',\n",
       " 'Âºµ',\n",
       " 'Ïñ¥',\n",
       " 'D',\n",
       " 'Áôª',\n",
       " 'Â∑ß',\n",
       " 'Êñπ',\n",
       " 'Â®Å',\n",
       " 'ÈÖí',\n",
       " '◊ê',\n",
       " 'Ëæâ',\n",
       " '·Éõ',\n",
       " 'Âä®',\n",
       " 'Œ®',\n",
       " 'Êë¶',\n",
       " 'Ïãù',\n",
       " 'Î†§',\n",
       " '¬•',\n",
       " 'ÏÑ∏',\n",
       " 'ÈÅì',\n",
       " 'Êßã',\n",
       " '◊•',\n",
       " 'Êµ©',\n",
       " 'Œß',\n",
       " '„É©',\n",
       " '‡∏°',\n",
       " 'Êéà',\n",
       " 'Áë∂',\n",
       " 'ÀÄ',\n",
       " 'Âá§',\n",
       " 'Á™Å',\n",
       " 'Ê¶ï',\n",
       " 'Â•ö',\n",
       " '¬´',\n",
       " 'ÂÆø',\n",
       " 'ÂÆ§',\n",
       " '‚Öì',\n",
       " 'Áî≥',\n",
       " 'Í∞ê',\n",
       " '„Éè',\n",
       " '„Éß',\n",
       " 'Ê©ò',\n",
       " 'Êúü',\n",
       " '‡™®',\n",
       " 'Îã§',\n",
       " 'Â∞é',\n",
       " 'Áüø',\n",
       " 'Ìõà',\n",
       " '·Ωº',\n",
       " 'Îûô',\n",
       " 'Î∞è',\n",
       " '·∫©',\n",
       " 'Á•®',\n",
       " 'ÌãÄ',\n",
       " '„Å±',\n",
       " 'Èôà',\n",
       " 'Ëæõ',\n",
       " '—ò',\n",
       " '·∫≥',\n",
       " 'ÁöÜ',\n",
       " 'ÌôÄ',\n",
       " 'Áõ£',\n",
       " 'Â§â',\n",
       " 'Èï∑',\n",
       " 'l',\n",
       " '–±',\n",
       " 'Êô¥',\n",
       " 'Áè†',\n",
       " 't',\n",
       " '⁄à',\n",
       " 'J',\n",
       " 'Â¨∏',\n",
       " '„Å´',\n",
       " 'ÈÉß',\n",
       " 'ÂºÄ',\n",
       " 'Áúü',\n",
       " 'Èòø',\n",
       " 'Áµê',\n",
       " 'Ëõã',\n",
       " 'Ê∏Ø',\n",
       " 'Âúñ',\n",
       " '„Å∂',\n",
       " 'œÅ',\n",
       " '„Åê',\n",
       " '·º¥',\n",
       " 'Îãµ',\n",
       " 'Âêõ',\n",
       " 'ËÉñ',\n",
       " 'Ê≠•',\n",
       " '„ÅÆ',\n",
       " 'ÊÇ™',\n",
       " 'Ê∏ù',\n",
       " 'Áîª',\n",
       " 'Î®∏',\n",
       " 'ÂÆ£',\n",
       " 'ÂÇô',\n",
       " '–∫',\n",
       " '·Ω†',\n",
       " 'Â§´',\n",
       " 'Âèô',\n",
       " 'ÔΩí',\n",
       " '‰æç',\n",
       " 'Èáã',\n",
       " 'Áî∫',\n",
       " 'ÿ±',\n",
       " 'Ï∞¨',\n",
       " 'Áª¥',\n",
       " 'ÊÄù',\n",
       " ' ä',\n",
       " '‰ºé',\n",
       " 'ÈúΩ',\n",
       " 'Ïóë',\n",
       " 'Ê•ä',\n",
       " 'Áé≤',\n",
       " 'Êà∞',\n",
       " 'j',\n",
       " 'Â∂ã',\n",
       " '¬∫',\n",
       " 'Ê©ü',\n",
       " 'Œ•',\n",
       " 'Â≥¥',\n",
       " 'Áèæ',\n",
       " 'ÂΩ∞',\n",
       " 'Èà¥',\n",
       " 'Èáé',\n",
       " 'Âáª',\n",
       " 'Ë©î',\n",
       " 'ÈΩä',\n",
       " 'È¨º',\n",
       " 'Ïó¨',\n",
       " '·É•',\n",
       " 'Ëü≤',\n",
       " '–ù',\n",
       " 'ËêΩ',\n",
       " 'ÂÅµ',\n",
       " 'Ë©©',\n",
       " '’∑',\n",
       " 'Ë≤†',\n",
       " 'Âüπ',\n",
       " 'Êäó',\n",
       " 'Ïªµ',\n",
       " '„Çß',\n",
       " 'Âûã',\n",
       " 'Ë≠¶',\n",
       " '‡§∏',\n",
       " 'Ëµ´',\n",
       " 'ÊÑà',\n",
       " 'ÂÆû',\n",
       " '–≠',\n",
       " 'Áê¥',\n",
       " 'Q',\n",
       " '‰º∏',\n",
       " 'Á®≤',\n",
       " '„Çº',\n",
       " 'Áà∫',\n",
       " 'Èô∏',\n",
       " 'Ê≠å',\n",
       " '2',\n",
       " '‰∫å',\n",
       " 'ÂøÖ',\n",
       " 'Âª∫',\n",
       " '·ø¥',\n",
       " 'ÁÜä',\n",
       " 'Êàí',\n",
       " 'Êâì',\n",
       " 'Ìïò',\n",
       " 'Áßë',\n",
       " 'Ï¶à',\n",
       " 'Êñé',\n",
       " 'Áæ©',\n",
       " 'Âãù',\n",
       " 'ÊµÅ',\n",
       " 'Œò',\n",
       " '’Å',\n",
       " '—â',\n",
       " '‡™è',\n",
       " 'Í∞ù',\n",
       " 'Î∞•',\n",
       " 'Ìï®',\n",
       " 'ÊΩÆ',\n",
       " 'Œô',\n",
       " 'Î¥ê',\n",
       " 'ÊÉß',\n",
       " 'ÂØπ',\n",
       " '·ö®',\n",
       " '‰∏ú',\n",
       " 'k',\n",
       " 'Â≠©',\n",
       " 'Ïò§',\n",
       " 'Ëä±',\n",
       " '‰ªä',\n",
       " 'È∫ó',\n",
       " 'Ôº§',\n",
       " 'Ôº´',\n",
       " 'Ê°à',\n",
       " 'Ëßà',\n",
       " 'Êóè',\n",
       " 'Â≤¨',\n",
       " 'Âºê',\n",
       " 'Â∑®',\n",
       " 'Ëä≥',\n",
       " '“Æ',\n",
       " 'ÏÑ¨',\n",
       " 'ÌÜ†',\n",
       " 'Á¥Ä',\n",
       " 'Ëøî',\n",
       " 'ÏßÅ',\n",
       " 'Áôæ',\n",
       " 'Âé≤',\n",
       " 'ËæΩ',\n",
       " '·ä¢',\n",
       " 'ÂãÉ',\n",
       " '·∏§',\n",
       " 'ÂèØ',\n",
       " '„Éé',\n",
       " 'Ìåê',\n",
       " 'ÌÑ∞',\n",
       " 'Â•ó',\n",
       " 'Ìïô',\n",
       " '‡§Ü',\n",
       " 'ÂÄÆ',\n",
       " 'Âüº',\n",
       " '„Çç',\n",
       " '≈ö',\n",
       " 'Áâô',\n",
       " 'Ÿá',\n",
       " 'Èùô',\n",
       " 'Áï∞',\n",
       " 'Èùí',\n",
       " 'Ïùº',\n",
       " '◊ö',\n",
       " 'Ôºí',\n",
       " 'Áªº',\n",
       " 'Ê≤ü',\n",
       " 'ƒª',\n",
       " 'Ïà†',\n",
       " '„É†',\n",
       " 'ÂÆù',\n",
       " 'ÂÑí',\n",
       " 'ÁÜô',\n",
       " ',',\n",
       " 'ËΩü',\n",
       " 'Áåì',\n",
       " 'Ôº≥',\n",
       " 'Âúã',\n",
       " '3',\n",
       " '”ô',\n",
       " 'Á£Å',\n",
       " '‘º',\n",
       " 'Âçö',\n",
       " 'Ìéò',\n",
       " '·ïø',\n",
       " '–ô',\n",
       " 'Êúâ',\n",
       " 'Ê¢Ö',\n",
       " 'I',\n",
       " 'Ë≤û',\n",
       " 'Ìòº',\n",
       " 'ÌåÖ',\n",
       " 'Èçæ',\n",
       " 'Âë≥',\n",
       " '–°',\n",
       " 'B',\n",
       " '·†Æ',\n",
       " 'œÉ',\n",
       " 'Êé•',\n",
       " '⁄Ü',\n",
       " '„Çã',\n",
       " 'Ë°Ä',\n",
       " 'ÊÄú',\n",
       " 'Ëåú',\n",
       " 'Ëå∂',\n",
       " '·ïê',\n",
       " '„Çª',\n",
       " 'Áöá',\n",
       " 'Âπ∂',\n",
       " '‰øó',\n",
       " '„Åë',\n",
       " 'Ìïú',\n",
       " 'Êå∫',\n",
       " 'Ë≥õ',\n",
       " 'Êàø',\n",
       " 'Ê∏°',\n",
       " 'Èå¢',\n",
       " 'Ëà∞',\n",
       " 'Â∑•',\n",
       " '‰∏ª',\n",
       " 'Êµ¶',\n",
       " 'Î¥â',\n",
       " 'Áõõ',\n",
       " '≈∂',\n",
       " 'F',\n",
       " 'Èì®',\n",
       " 'ÂíÑ',\n",
       " '·æ∂',\n",
       " '—ó',\n",
       " 'Ëºù',\n",
       " 'Êºî',\n",
       " 'Í∞ë',\n",
       " '„Åã',\n",
       " 'Îü¨',\n",
       " 'Í∏à',\n",
       " 'Î∞±',\n",
       " 'Ï†ú',\n",
       " '·õÉ',\n",
       " 'Âßú',\n",
       " '#',\n",
       " 'ÏÑú',\n",
       " 'ÎπÖ',\n",
       " '„Å∏',\n",
       " 'ÍÄí',\n",
       " 'Êâπ',\n",
       " 'Ìè¨',\n",
       " 'ÂÜ≤',\n",
       " '„ÇÆ',\n",
       " '1',\n",
       " 'Ï∂ú',\n",
       " 'Ê¨¢',\n",
       " 'Ëîµ',\n",
       " 'ÂÑÑ',\n",
       " 'ÁæΩ',\n",
       " 'Êóã',\n",
       " 'Ôº±',\n",
       " '‰Ωô',\n",
       " 'Âá∫',\n",
       " 'Èº†',\n",
       " 'œà',\n",
       " '„Éä',\n",
       " 'È∏£',\n",
       " 'Áõ§',\n",
       " 'Ïñë',\n",
       " 'ÂÇ≥',\n",
       " 'Ïî®',\n",
       " 'Ïàú',\n",
       " 'Ë£ï',\n",
       " 'Ëê¨',\n",
       " '„Éá',\n",
       " 'Ê¢Å',\n",
       " '◊ß',\n",
       " '‡§®',\n",
       " 'Â∏å',\n",
       " '‰ºû',\n",
       " 'Ï¢ã',\n",
       " '·∏¥',\n",
       " '‰∫Æ',\n",
       " 'Ê∏©',\n",
       " 'Ïûò',\n",
       " '„ÇΩ',\n",
       " '‚Äî',\n",
       " 'n',\n",
       " '‰ºè',\n",
       " 'ÔΩà',\n",
       " 'È£û',\n",
       " 'Î∏î',\n",
       " 'Ìï©',\n",
       " '‡∏û',\n",
       " 'ŸÜ',\n",
       " 'Z',\n",
       " 'Èò™',\n",
       " 'Â∫¶',\n",
       " 'ËØÑ',\n",
       " 'ÍπÄ',\n",
       " 'Î•∏',\n",
       " '·É¢',\n",
       " 'Îã¨',\n",
       " '‰π¶',\n",
       " '…í',\n",
       " '‰Ωõ',\n",
       " 'ÁÆ±',\n",
       " 'ÊΩ§',\n",
       " 'Ïó¥',\n",
       " 'Âªª',\n",
       " 'Èπè',\n",
       " 'ÀÅ',\n",
       " 'Â±Ö',\n",
       " 'ÂçÉ',\n",
       " 'È∫©',\n",
       " 'È¶Ü',\n",
       " 'Â∏∂',\n",
       " 'Á•ê',\n",
       " 'Ëµ∞',\n",
       " 'ÊóÖ',\n",
       " 'Á≠ë',\n",
       " 'Êåâ',\n",
       " 'Âµ©',\n",
       " 'W',\n",
       " '“õ',\n",
       " 'E',\n",
       " 'Â∑¢',\n",
       " 'Í∞ï',\n",
       " 'f',\n",
       " 'Ê∏Ö',\n",
       " 'È∫∫',\n",
       " 'Ôºô',\n",
       " '„É¨',\n",
       " 'Ï≤≠',\n",
       " '◊ñ',\n",
       " 'Ïã†',\n",
       " 'Î∂Ñ',\n",
       " 'Î≤Ñ',\n",
       " 'Êúç',\n",
       " 'Ï†ï',\n",
       " 'Ê¨°',\n",
       " '·ñÖ',\n",
       " 'Ë™å',\n",
       " 'ÊÆ∑',\n",
       " 'Ï°∞',\n",
       " 'Ëôö',\n",
       " 'Â£¨',\n",
       " 'Ê¢ù',\n",
       " 'Ë™¨',\n",
       " 'Ê†ë',\n",
       " '„Å≠',\n",
       " 'ÏïΩ',\n",
       " '„Çπ',\n",
       " 'Êîæ',\n",
       " 'Áúã',\n",
       " '’§',\n",
       " '—ú',\n",
       " 'ÏΩî',\n",
       " 'ËåÖ',\n",
       " 'Ë®ä',\n",
       " 'ÈÄö',\n",
       " '5',\n",
       " 'È•Ö',\n",
       " 'Â≠ô',\n",
       " 'Êãç',\n",
       " '‰ºç',\n",
       " ' º',\n",
       " 'Ïπº',\n",
       " 'Ìï≠',\n",
       " 'Áâá',\n",
       " 'Ëî£',\n",
       " 'g',\n",
       " 'Ëæº',\n",
       " '‡∏•',\n",
       " 'Î£å',\n",
       " 'Ê±†',\n",
       " 'ÂÖÉ',\n",
       " 'ÎåÄ',\n",
       " 'Á¨î',\n",
       " 'Â£´',\n",
       " 'È°æ',\n",
       " 'Ê†Ñ',\n",
       " 'Êá∑',\n",
       " 'Ïûê',\n",
       " '‰∏π',\n",
       " 'Ëæë',\n",
       " 'Ë±π',\n",
       " 'Áçé',\n",
       " 'Ëï≠',\n",
       " 'Á¶è',\n",
       " 'Ëá®',\n",
       " 'Áã±',\n",
       " 'Á¥∫',\n",
       " 'Í±∏',\n",
       " 'ÏΩ§',\n",
       " '‰∫û',\n",
       " '◊™',\n",
       " 'Ïïî',\n",
       " '—ô',\n",
       " '‰∏ã',\n",
       " 'ÈÑô',\n",
       " 'Ë©¶',\n",
       " 'Áä∂',\n",
       " 'Ê∂ô',\n",
       " 'Î≤ï',\n",
       " 'ƒé',\n",
       " 'Œú',\n",
       " 'Î∞§',\n",
       " '’Ñ',\n",
       " 'b',\n",
       " 'ËØÜ',\n",
       " 'Â∫ï',\n",
       " '·ºÄ',\n",
       " 'Êêû',\n",
       " 'Â∏∫',\n",
       " 'ÊØì',\n",
       " 'Ê†º',\n",
       " 'Èæô',\n",
       " 'ÌÉÑ',\n",
       " 'Âªâ',\n",
       " '‡∏á',\n",
       " 'Á¥î',\n",
       " '·º∞',\n",
       " 'ÈåØ',\n",
       " 'Âçè',\n",
       " '«ö',\n",
       " '„Çä',\n",
       " 'Îßå',\n",
       " 'ÂÆã',\n",
       " 'ÈìÅ',\n",
       " 'Âéø',\n",
       " 'Ï¶å',\n",
       " 'Âç®',\n",
       " '·âΩ',\n",
       " 'Èáç',\n",
       " 'Ëá£',\n",
       " 'Èô¢',\n",
       " '„Å•',\n",
       " 'Îãõ',\n",
       " 'Âßê',\n",
       " 'Â∞Å',\n",
       " '„Å§',\n",
       " 'Âí¨',\n",
       " 'Ë£Ω',\n",
       " 'Èßõ',\n",
       " '·º©',\n",
       " 'Â†Ç',\n",
       " 'Ëî°',\n",
       " 'Êùæ',\n",
       " 'Ëìù',\n",
       " 'Ë¶∫',\n",
       " 'Âùä',\n",
       " 'Èôç',\n",
       " 'Â∫Ñ',\n",
       " '’æ',\n",
       " 'Ôºì',\n",
       " 'Ê•∑',\n",
       " 'ÈΩí',\n",
       " 'Èóª',\n",
       " 'ÂÅè',\n",
       " 'Á∫ß',\n",
       " 'Œ†',\n",
       " 'Èü©',\n",
       " '‰Ωè',\n",
       " '„Çà',\n",
       " 'Áß∞',\n",
       " 'Ï≤†',\n",
       " 'Âõ∫',\n",
       " 'Â∏´',\n",
       " 'Âãï',\n",
       " '◊î',\n",
       " '·Ω∫',\n",
       " '„Åù',\n",
       " '–†',\n",
       " '’ª',\n",
       " 'Êù•',\n",
       " '‘¥',\n",
       " '·†≤',\n",
       " '’≥',\n",
       " 'Œù',\n",
       " 'ÏúÑ',\n",
       " 'Áëú',\n",
       " 'Î°ù',\n",
       " 'Á¢∫',\n",
       " 'Êù∞',\n",
       " 'ÈÉ∑',\n",
       " 'Áïå',\n",
       " 'Â∞ñ',\n",
       " 'Âä©',\n",
       " 'ÏºÄ',\n",
       " 'T',\n",
       " 'Ê≤¢',\n",
       " '·àµ',\n",
       " '„Åò',\n",
       " 'Áúº',\n",
       " 'ÎÖ∏',\n",
       " 'Ëôû',\n",
       " 'Ï∂ï',\n",
       " '–∏',\n",
       " 'Èòü',\n",
       " 'È∫≠',\n",
       " 'Á†î',\n",
       " 'Â∫´',\n",
       " 'Âºæ',\n",
       " 'œÑ',\n",
       " '„Ç≤',\n",
       " 'Êæé',\n",
       " 'Y',\n",
       " '·Éò',\n",
       " 'Œπ',\n",
       " 'Êòì',\n",
       " 'È®∞',\n",
       " 'Âπ≥',\n",
       " 'Áßò',\n",
       " 'ÎÑ§',\n",
       " 'ÊäÑ',\n",
       " 'Áõº',\n",
       " 'Âëº',\n",
       " 'ÈØë',\n",
       " 'Îâ¥',\n",
       " 'Ë•≤',\n",
       " 'Á•Å',\n",
       " 'Âíå',\n",
       " 'Ïóò',\n",
       " 'ÈØñ',\n",
       " 'Áøº',\n",
       " '‰Ωç',\n",
       " '–®',\n",
       " 'Ê£Æ',\n",
       " 'Âπï',\n",
       " 'ÂΩπ',\n",
       " 'Íµ∞',\n",
       " '◊§',\n",
       " 'Ëàá',\n",
       " 'Áãº',\n",
       " 'Ïö©',\n",
       " 'Ê∞¥',\n",
       " 'Âùõ',\n",
       " 'Êà≤',\n",
       " 'ÂãÅ',\n",
       " 'Âä†',\n",
       " 'œÖ',\n",
       " '‰Ωï',\n",
       " 'ÁΩó',\n",
       " 'Ìò∏',\n",
       " 'ÂΩì',\n",
       " 'Â±±',\n",
       " 'Êøü',\n",
       " 'Ë±Ü',\n",
       " 'ÊµÖ',\n",
       " 'ÁªÑ',\n",
       " '·ΩÑ',\n",
       " '·âµ',\n",
       " 'Ë∫´',\n",
       " 'ÁÇí',\n",
       " 'Èöà',\n",
       " 'Îç∞',\n",
       " 'ÈõÖ',\n",
       " 'ÂØæ',\n",
       " '>',\n",
       " 'Áõä',\n",
       " 'Âπø',\n",
       " '‡§≥',\n",
       " '–Å',\n",
       " 'Áïô',\n",
       " 'Êñ¨',\n",
       " 'Â∫≠',\n",
       " 'Îïå',\n",
       " 'Âµê',\n",
       " 'a',\n",
       " 'ŒΩ',\n",
       " '–¢',\n",
       " 'Áúû',\n",
       " 'Â±Ä',\n",
       " 'Èôê',\n",
       " 'Ë™≤',\n",
       " 'ÊÜê',\n",
       " '·É®',\n",
       " 'ÂΩï',\n",
       " 'ÿ¢',\n",
       " 'Êôì',\n",
       " 'Ë≤°',\n",
       " '‰∏ä',\n",
       " 'ÊîÄ',\n",
       " 'ŸÖ',\n",
       " 'Ïùë',\n",
       " 'Ááü',\n",
       " 'ÁÅØ',\n",
       " 'Ïôï',\n",
       " 'Ïó≠',\n",
       " '·øá',\n",
       " '·ª≠',\n",
       " 'Á∫µ',\n",
       " 'ËÑà',\n",
       " 'ƒ™',\n",
       " 'Âª£',\n",
       " 'Á¢ß',\n",
       " 'Êßª',\n",
       " 'Ìëú',\n",
       " 'Ìé∏',\n",
       " 'Á¶Æ',\n",
       " '‰∏ñ',\n",
       " 'ÂÑÄ',\n",
       " 'Êæ§',\n",
       " 'Ëµµ',\n",
       " 'ÿ•',\n",
       " 'ÏÑù',\n",
       " 'Ëñá',\n",
       " 'Áù°',\n",
       " '◊ì',\n",
       " 'Âà∏',\n",
       " '·ΩÄ',\n",
       " 'Â§¥',\n",
       " 'ËÉ•',\n",
       " 'Â∏à',\n",
       " 'È≥≥',\n",
       " 'Ôºê',\n",
       " 'ÁõÆ',\n",
       " '–Ø',\n",
       " 'ÁÉü',\n",
       " 'Áªò',\n",
       " 'Â¶≤',\n",
       " 'ÊïÖ',\n",
       " '¬µ',\n",
       " 'Œ∑',\n",
       " 'Êûú',\n",
       " 'Âéã',\n",
       " 'ÌÉÄ',\n",
       " 'ÂΩô',\n",
       " 'ÊÄß',\n",
       " 'Ê£ü',\n",
       " '7',\n",
       " 'Î°±',\n",
       " 'Êòæ',\n",
       " 'Í∞Ñ',\n",
       " 'Î≤î',\n",
       " '·ªó',\n",
       " 'Âôå',\n",
       " '-',\n",
       " 'Îëê',\n",
       " 'Â™Ω',\n",
       " '·ø≥',\n",
       " 'ÈÇ†',\n",
       " 'Ë¥ü',\n",
       " 'Êàö',\n",
       " '„Öè',\n",
       " 'Âà©',\n",
       " 'Ê°ú',\n",
       " 'Âçî',\n",
       " 'Â≠´',\n",
       " ' É',\n",
       " 'ÊòÜ',\n",
       " 'ÊÄñ',\n",
       " '‰∫°',\n",
       " '„ÉÅ',\n",
       " 'ÁÑâ',\n",
       " 'Ê©ã',\n",
       " '‡∏™',\n",
       " 'Á∏Ñ',\n",
       " '„Åª',\n",
       " 'Áâπ',\n",
       " ' î',\n",
       " 'Êùø',\n",
       " '–ì',\n",
       " '–™',\n",
       " '…Ö',\n",
       " '·ãµ',\n",
       " 'Ïµú',\n",
       " 'Êùé',\n",
       " 'Âêà',\n",
       " 'ÂÜ†',\n",
       " 'ÂÖã',\n",
       " 'Îîî',\n",
       " 'Áæä',\n",
       " 'Ë¢Å',\n",
       " '„ÉÜ',\n",
       " '‡∏≤',\n",
       " 'Ëå≤',\n",
       " '¬æ',\n",
       " '‡∏ñ',\n",
       " 'Ê∏£',\n",
       " 'Èó´',\n",
       " 'È§Ö',\n",
       " 'Ôº¢',\n",
       " 'Î≥Ñ',\n",
       " 'ËÄÉ',\n",
       " '·Éó',\n",
       " 'Â£Å',\n",
       " 'ÈÅ∏',\n",
       " 'ÈÄÄ',\n",
       " '÷Ö',\n",
       " 'ƒ∂',\n",
       " 'ÿ∏',\n",
       " 'Ï°å',\n",
       " '·††',\n",
       " 'Ìåå',\n",
       " 'È¥ª',\n",
       " 'Âêç',\n",
       " 'Îπô',\n",
       " 'ÊÆ∫',\n",
       " 'ÊÅê',\n",
       " 'Áå¥',\n",
       " 'Êèê',\n",
       " '‰ºê',\n",
       " 'Îπµ',\n",
       " '’Ø',\n",
       " 'ÏÉâ',\n",
       " '„Éâ',\n",
       " '·ªÅ',\n",
       " 'ËÖæ',\n",
       " 'Á´†',\n",
       " 'Áé¢',\n",
       " 'Â§ú',\n",
       " 'Î≥¥',\n",
       " '—ã',\n",
       " '≈ª',\n",
       " '„Åß',\n",
       " 'Èªí',\n",
       " '‰Ωµ',\n",
       " 'ÊúØ',\n",
       " 'Â∏Ç',\n",
       " 'Êûó',\n",
       " 'ÌÜ®',\n",
       " 'Êä§',\n",
       " '·â∂',\n",
       " '·∫•',\n",
       " '·Éú',\n",
       " 'Â∏¶',\n",
       " 'ÂÖà',\n",
       " 'ÁØÑ',\n",
       " 'Á¢ü',\n",
       " 'Ê≠¶',\n",
       " 'Ïã§',\n",
       " '‰ºù',\n",
       " '‘Ω',\n",
       " 'Í∏∞',\n",
       " 'ÏÜê',\n",
       " 'ËØï',\n",
       " 'Ìä∏',\n",
       " 'ÏôÄ',\n",
       " 'Îêê',\n",
       " '·ä†',\n",
       " 'Èõë',\n",
       " '√©',\n",
       " '‰ªñ',\n",
       " '‰º¶',\n",
       " 'ŸÉ',\n",
       " 'Êï∏',\n",
       " 'Ëµõ',\n",
       " 'Î≥º',\n",
       " 'ËÆ∞',\n",
       " '„Üç',\n",
       " 'ÏÜå',\n",
       " '„Åé',\n",
       " 'Î∞ú',\n",
       " '¬™',\n",
       " 'Êù±',\n",
       " 'œâ',\n",
       " '(',\n",
       " 'Îøê',\n",
       " 'ÈÑ≠',\n",
       " 'Ëô¨',\n",
       " 'Êìî',\n",
       " 'ËÄÄ',\n",
       " 'Ë≠ò',\n",
       " '·ºº',\n",
       " 'Á¢â',\n",
       " 'ÊÅã',\n",
       " '·àã',\n",
       " 'Í≤Ω',\n",
       " '‰∫®',\n",
       " 'Êú™',\n",
       " 'ÂÜô',\n",
       " '∆è',\n",
       " 'Áø∞',\n",
       " 'Ë°õ',\n",
       " '·à≠',\n",
       " 'Â±Ñ',\n",
       " '„É£',\n",
       " 'Èô©',\n",
       " '–ë',\n",
       " 'Á¨∫',\n",
       " 'Èáå',\n",
       " 'Âçï',\n",
       " 'p',\n",
       " '·à´',\n",
       " 'Áêµ',\n",
       " '„ÇÅ',\n",
       " 'Ïã∂',\n",
       " '’∫',\n",
       " '„ÅΩ',\n",
       " 'Ë≥¶',\n",
       " 'q',\n",
       " 'Â∫è',\n",
       " 'Èúä',\n",
       " 'Áã∏',\n",
       " 'Ÿæ',\n",
       " 'Êõæ',\n",
       " 'Êúà',\n",
       " 'ÊíÉ',\n",
       " 'Êô∫',\n",
       " '‡πÅ',\n",
       " 'Ëâ≤',\n",
       " 'Ôº™',\n",
       " 'ÂÅ∂',\n",
       " 'Ìòë',\n",
       " '„Åè',\n",
       " '„Öì',\n",
       " '·ö±',\n",
       " '·Éö',\n",
       " 'ÂÜÜ',\n",
       " 'Âû£',\n",
       " 'Âºü',\n",
       " 'Ïóê',\n",
       " '…ú',\n",
       " '„Å†',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3080\n"
     ]
    }
   ],
   "source": [
    "words=df_x.iloc[:,1]\n",
    "print (len(words))\n",
    "print( type(words))\n",
    "char_list=[]\n",
    "\n",
    "import unicodedata\n",
    "for word in words:\n",
    "        temp_list=list(word)\n",
    "        char_list += temp_list\n",
    "        \n",
    "char_list=list(set(char_list))\n",
    "display(char_list)\n",
    "print (len(char_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Vocabulary count and assigning each character a numeric value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3080\n"
     ]
    }
   ],
   "source": [
    "# VOCAB COUNT\n",
    "\n",
    "VOCAB=char_list\n",
    "VOCAB_SIZE= len(VOCAB)\n",
    "print (VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict={}\n",
    "length=len(char_list)\n",
    "for i in range (length):\n",
    "    mydict[char_list[i]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict[' ']=length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_unique=len(char_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "hot_vec= np.zeros((VOCAB_SIZE,VOCAB_SIZE),int)\n",
    "np.fill_diagonal(hot_vec,1)\n",
    "hot_vec_dict={}\n",
    "\n",
    "index=0\n",
    "for word in VOCAB:\n",
    "    hot_vec_dict[word]=hot_vec[index].reshape(1,VOCAB_SIZE)\n",
    "    index+=1\n",
    "    \n",
    "#assign \"space\" a vec of zero\n",
    "hot_vec_dict[\" \"]=np.zeros((1,VOCAB_SIZE),int)\n",
    "print (len(hot_vec_dict))\n",
    "display(hot_vec_dict[\" \"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making One-Hot-Vectors for our classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484145\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ADDRESS': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'CARDINAL': array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'DATE': array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'DECIMAL': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'DIGIT': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " 'FRACTION': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       " 'LETTERS': array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'MEASURE': array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'MONEY': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
       " 'ORDINAL': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " 'PLAIN': array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'PUNCT': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'TELEPHONE': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " 'TIME': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
       " 'VERBATIM': array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes_list=df_x.iloc[:,0]\n",
    "class_hot_vec\n",
    "ind=0\n",
    "class_hotvec_dict={}\n",
    "for clas in unique_classes:\n",
    "    class_hotvec_dict[clas]=class_hot_vec[ind]\n",
    "    ind+=1\n",
    "    \n",
    "\n",
    "print (len(classes_list))\n",
    "print (len(class_hotvec_dict))\n",
    "display(class_hotvec_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of YTRAIN:  484145\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YTRAIN=[]\n",
    "for clas in classes_list:\n",
    "    YTRAIN.append(class_hotvec_dict[clas])\n",
    "\n",
    "print (\"len of YTRAIN: \",len(YTRAIN))\n",
    "print (YTRAIN[10])\n",
    "YTRAIN[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words:  484145\n"
     ]
    }
   ],
   "source": [
    "#del df_x\n",
    "print(\"Total Words: \",len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Vectors out of each word from our test data and adding padding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=0\n",
    "final_dict={}\n",
    "final_word=[]\n",
    "for word in words:\n",
    "    length=len(word)\n",
    "    array=[]\n",
    "    if max_length<length:     \n",
    "        max_length=length\n",
    "    for i in range(length):\n",
    "        array.append(mydict[word[i]])\n",
    "    zeros=0\n",
    "    if max_word_len>length:\n",
    "        l=max_word_len-length\n",
    "        zeros=[]\n",
    "        for i in range(l):\n",
    "            zeros.append(length_of_unique)\n",
    "        #print (zeros)\n",
    "        zeros.extend(array)\n",
    "        final_word.append(zeros)\n",
    "    else:\n",
    "        final_word.append(array)\n",
    "    #print (zeros)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(484145, 323)\n"
     ]
    }
   ],
   "source": [
    "final_word=np.array(final_word)\n",
    "print (final_word.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print (\"Len of Ytrain:\", len(YTRAIN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing One-Hot-Vec's for Lables to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=\"C:\\\\Users\\\\Dani-Desktop\\\\Desktop\\\\NLP DATA\\\\alldata\\\\y_train\"\n",
    "np.savez_compressed(outfile, YTRAIN=YTRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading lables vectors from file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YTRAIN']\n",
      "Files Read\n"
     ]
    }
   ],
   "source": [
    "outfile2=\"C:\\\\Users\\\\Dani-Desktop\\\\Desktop\\\\NLP DATA\\\\alldata\\\\y_train.npz\"\n",
    "npzfile = np.load(outfile2)\n",
    "print (npzfile.files)\n",
    "#x_train=npzfile['XTRAIN']\n",
    "x_train_f=final_word\n",
    "y_train_f=npzfile['YTRAIN']\n",
    "npzfile.close()\n",
    "print(\"Files Read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(484145, 323)\n",
      "(484145, 15)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_f.shape)\n",
    "print(y_train_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df_c\n",
    "del df_x\n",
    "del YTRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an LSTM model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(13)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the data into Test and Training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387316, 323)\n",
      "(387316, 15)\n",
      "(96827, 323)\n",
      "(96827, 15)\n"
     ]
    }
   ],
   "source": [
    "max_features = lenx\n",
    "x_train=x_train_f[:387316]\n",
    "y_train=y_train_f[:387316]\n",
    "x_test=x_train_f[387317:-1]\n",
    "y_test=y_train_f[387317:-1]\n",
    "maxlen = max_word_len\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an LSTM model with Softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 323, 10)           30810     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 323, 20)           2480      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 20)                3280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                315       \n",
      "=================================================================\n",
      "Total params: 36,885\n",
      "Trainable params: 36,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=VOCAB_SIZE+1, output_dim=10, input_length=max_word_len))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(LSTM(20, return_sequences=False))\n",
    "model.add(Dense(15, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "filepath=\"C:\\\\Users\\\\Dani-Desktop\\\\Desktop\\\\NLP DATA\\\\alldata\\\\weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 387316 samples, validate on 96827 samples\n",
      "Epoch 1/10\n",
      "387264/387316 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9324Epoch 00000: val_loss improved from inf to 0.12005, saving model to C:\\Users\\Dani-Desktop\\Desktop\\NLP DATA\\alldata\\weights-improvement-00-0.2250.hdf5\n",
      "387316/387316 [==============================] - 2196s - loss: 0.2250 - acc: 0.9324 - val_loss: 0.1201 - val_acc: 0.9565\n",
      "Epoch 2/10\n",
      " 55744/387316 [===>..........................] - ETA: 1745s - loss: 0.1178 - acc: 0.9610"
     ]
    }
   ],
   "source": [
    "res=model.fit(x_train,y_train,batch_size=64,validation_data=(x_test,y_test),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Loss and Accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(epochs)\n",
    "plt.plot(x, res.history['acc'], label='LSTM train')\n",
    "plt.plot(x, res.history['val_acc'], label='LSTM val')\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, res.history['loss'], label='LSTM train')\n",
    "plt.plot(x, res.history['val_loss'], label='LSTM val')\n",
    "plt.title('Loss')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
